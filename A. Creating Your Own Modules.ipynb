{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"A. Creating Your Own Modules.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"fMOwH9brEKjG","colab_type":"text"},"source":["## PyTorch Tutorial\n","\n","IFT6135 â€“ Representation Learning\n","\n","A Deep Learning Course, January 2020\n","\n","By Chin-Wei Huang \n","\n","(Adapted from Sandeep Subramanian's MILA tutorial)"]},{"cell_type":"markdown","metadata":{"id":"S9-ADhPtEKjI","colab_type":"text"},"source":["## Creating Your Own Modules\n","\n","### `torch.nn.module`"]},{"cell_type":"code","metadata":{"id":"ShtBh1MkEKjI","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnQSNC-9_clw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2g20GUADEKjL","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","\n","import math\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TEzrrZQcEKjN","colab_type":"text"},"source":["``nn.Module`` is base class for all neural network modules.\n","\n","You should also write your modules as sub-class of ``nn.Module``, so that it can inherit the following attributes:\n","\n","* *Recursive structure*: you can wrap an instantiation of a Module class with another one, which stores the inner one as its parent\n","\n","* *Cudafiability*: you can easily cudafy the whole sequence of modules using `model.cuda()`\n","\n","* *Serializable*: you can save your trained model (checkpoint, early stopping ...) using ``torch.save``, ``torch.load``\n","\n","* *Parameters*: you can call model.parameters() to access all parameters at the same time. \n","\n","etc. \n"]},{"cell_type":"code","metadata":{"id":"Be3_EWPkEKjO","colab_type":"code","colab":{}},"source":["# modified from https://pytorch.org/docs/master/_modules/torch/nn/modules/linear.html#Linear\n","class Linear(nn.Module):\n","    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n","\n","    Args:\n","        in_features: size of each input sample\n","        out_features: size of each output sample\n","        bias: If set to False, the layer will not learn an additive bias. Default: True\n","\n","    Shape:\n","        - Input: :math:`(N, in\\_features)`\n","        - Output: :math:`(N, out\\_features)`\n","\n","    ...\n","\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(Linear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.Tensor(out_features, in_features))\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        bound = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-bound, bound)\n","        if self.bias is not None:\n","            self.bias.data.zero_()\n","\n","    def forward(self, input):\n","        return F.linear(input, self.weight, self.bias)\n","\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZb6OPjaEKjQ","colab_type":"code","colab":{}},"source":["class MyLinear(nn.Module):\n","    \n","    def __init__(self, in_features, out_features, bias=True):\n","        super(MyLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.Tensor(in_features, out_features))\n","        if bias:\n","            self.bias = Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","    def forward(self, input):\n","        if self.bias is None:\n","            return torch.mm(input, self.weight) \n","        else:\n","            return torch.mm(input, self.weight) + self.bias\n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7z9kGf_EKjS","colab_type":"code","outputId":"9738ddd9-fdbc-4bae-9e0f-f6dcd66d6f8e","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1579496551636,"user_tz":300,"elapsed":389,"user":{"displayName":"Chin-Wei Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0YZN_UP_2Nf2Ega66XRR1oJwSHZ_x3lXywxoamw=s64","userId":"09413186390566970719"}}},"source":["x = torch.from_numpy(np.random.randn(2, 3)).float()\n","\n","\n","linear1 = nn.Linear(3,4)\n","linear2 = MyLinear(3,4)\n","\n","# set the weight and bias of linear2 to be the same as linear1's\n","linear2.weight.data = linear1.weight.data.transpose(1,0)\n","linear2.bias.data = linear1.bias.data\n","\n","print(torch.eq(linear1(x), linear2(x)))\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([[True, True, True, True],\n","        [True, True, True, True]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qxKupGdcEKjU","colab_type":"text"},"source":["### Resnet example\n","\n","* Resnet blocks let the gradient flow through the hidden unit more directly and at the same time increase expressiveness\n","\n","Res(x) = F(x, {W}) + x\n","\n","Res(x) = F(x, {W1}) + W2 x\n","\n"]},{"cell_type":"code","metadata":{"id":"kJI6vKXREKjV","colab_type":"code","colab":{}},"source":["# Q: write the resnet block which projects the data properly if in_feat != out_fest\n","class ResLinear(nn.Module):\n","\n","    def __init__(self, in_features, out_features, activation=nn.ReLU()):\n","        super(ResLinear, self).__init__()\n","        # write your code here\n","        pass\n","    def forward(self, x):\n","        # write your code here\n","        pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-rd-3PCEKjX","colab_type":"code","outputId":"55c4b8b7-d048-4b36-83ad-498090779086","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1579496849922,"user_tz":300,"elapsed":589,"user":{"displayName":"Chin-Wei Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0YZN_UP_2Nf2Ega66XRR1oJwSHZ_x3lXywxoamw=s64","userId":"09413186390566970719"}}},"source":["x = torch.from_numpy(np.random.randn(2, 3)).float()\n","\n","\n","res1 = nn.Linear(3,3)\n","res2 = ResLinear(3,5)\n","\n","print(res1(x).size())\n","print(res2(x).size())"],"execution_count":16,"outputs":[{"output_type":"stream","text":["torch.Size([2, 3])\n","torch.Size([2, 5])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UcYZINd7EKjZ","colab_type":"text"},"source":["### Putting things altogether, Sequential, Parameter updates"]},{"cell_type":"code","metadata":{"id":"MytMXwn8EKjZ","colab_type":"code","colab":{}},"source":["class MyModel(nn.Module):\n","    \n","    def __init__(self, Linear=ResLinear):\n","        super(MyModel, self).__init__()\n","        \n","        self.predict_ = nn.Sequential(\n","            Linear(784, 328),\n","            nn.ReLU(),\n","            Linear(328, 328),\n","            nn.ReLU(),\n","            Linear(328, 10),\n","        )\n","        \n","        \n","        self.criterion = nn.CrossEntropyLoss()\n","    \n","    def predict_proba(self, x):\n","        return F.softmax(x)\n","    \n","    def predict(self, x):\n","        return torch.max(self.predict_proba(x))[1]\n","        # ``max'' returns (max_value, argmax)\n","    \n","    def loss(self, x, target):\n","        proba = self.predict_(x)\n","        return self.criterion(proba, target)\n","\n","# CrossEntropyLoss -> output no nonlinearity\n","# NLLloss -> logsoftmax\n","        \n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYI0ifSxEKjb","colab_type":"text"},"source":["caveate:    ``CrossEntropyLoss``    versus    ``NLLLoss``\n","\n","\n","* ``CrossEntropyLoss`` takes in *pre-softmax* as input\n","\n","* ``NLLLoss`` takes in *log-softmax* as input\n"]},{"cell_type":"code","metadata":{"id":"WYUTmME8EKjc","colab_type":"code","outputId":"7cfe2833-487c-4668-fd1b-e73e02a8d6e5","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1579496917116,"user_tz":300,"elapsed":360,"user":{"displayName":"Chin-Wei Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0YZN_UP_2Nf2Ega66XRR1oJwSHZ_x3lXywxoamw=s64","userId":"09413186390566970719"}}},"source":["y = torch.Tensor(1,10).normal_() # logit\n","t = torch.from_numpy(np.random.choice(10, size=1))\n","\n","loss1 = nn.CrossEntropyLoss()\n","loss2 = nn.NLLLoss()\n","\n","print(loss1(y, t))\n","print(loss2(nn.LogSoftmax(dim=1)(y), t))\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["tensor(2.1038)\n","tensor(2.1038)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qaEpEeqOEKje","colab_type":"code","outputId":"945bfe64-a631-4afa-977e-7577d1144f86","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1579496930470,"user_tz":300,"elapsed":562,"user":{"displayName":"Chin-Wei Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0YZN_UP_2Nf2Ega66XRR1oJwSHZ_x3lXywxoamw=s64","userId":"09413186390566970719"}}},"source":["x = torch.from_numpy(np.random.randn(64, 784)).float()\n","t = torch.from_numpy(np.random.choice(10, size=64))\n","\n","model = MyModel()\n","print(model.loss(x, t))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["tensor(2.4775, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hGNCoEATEKjg","colab_type":"text"},"source":["### Updating Parameters (Manually)"]},{"cell_type":"code","metadata":{"id":"PqAM1cz7EKjg","colab_type":"code","outputId":"5ed49112-078c-4c03-cb25-934a53ef636c","colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"status":"ok","timestamp":1579497197205,"user_tz":300,"elapsed":553,"user":{"displayName":"Chin-Wei Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0YZN_UP_2Nf2Ega66XRR1oJwSHZ_x3lXywxoamw=s64","userId":"09413186390566970719"}}},"source":["x = torch.from_numpy(np.random.randn(64, 784)).float()\n","t = torch.from_numpy(np.random.choice(10, size=64))\n","model = MyModel()\n","\n","lr = 0.1\n","\n","for i in range(10):\n","    loss = model.loss(x, t)\n","    loss.backward()\n","    \n","    for param in model.parameters():\n","        # param.data = param.data - lr*param.grad.data\n","        param.data.sub_(param.grad.data*lr)\n","        param.grad.data.zero_()\n","        \n","    print(loss.item())\n","    \n","    # print(param.grad)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["2.4042789936065674\n","1.5861577987670898\n","1.0716038942337036\n","0.7304816246032715\n","0.47881975769996643\n","0.3232457637786865\n","0.23248334228992462\n","0.17822784185409546\n","0.14187084138393402\n","0.11654175817966461\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4hmGXxjAEKji","colab_type":"text"},"source":["### Updating Parameters (``torch.optim``)"]},{"cell_type":"code","metadata":{"id":"mRulHUBDEKjj","colab_type":"code","outputId":"5964e6d3-8e4f-4c03-9115-c9ccb95c325e","colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"status":"ok","timestamp":1579497224471,"user_tz":300,"elapsed":704,"user":{"displayName":"Chin-Wei Huang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB0YZN_UP_2Nf2Ega66XRR1oJwSHZ_x3lXywxoamw=s64","userId":"09413186390566970719"}}},"source":["x = torch.from_numpy(np.random.randn(64, 784)).float()\n","t = torch.from_numpy(np.random.choice(10, size=64))\n","model = MyModel()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.0)\n","\n","\n","for i in range(10):\n","    optimizer.zero_grad()\n","    \n","    loss = model.loss(x, t)\n","    loss.backward()\n","    \n","    optimizer.step()\n","        \n","    print(loss)\n","\n","# what happens if you use momentum? "],"execution_count":38,"outputs":[{"output_type":"stream","text":["tensor(2.4058, grad_fn=<NllLossBackward>)\n","tensor(1.5623, grad_fn=<NllLossBackward>)\n","tensor(1.0356, grad_fn=<NllLossBackward>)\n","tensor(0.7061, grad_fn=<NllLossBackward>)\n","tensor(0.4813, grad_fn=<NllLossBackward>)\n","tensor(0.3393, grad_fn=<NllLossBackward>)\n","tensor(0.2466, grad_fn=<NllLossBackward>)\n","tensor(0.1892, grad_fn=<NllLossBackward>)\n","tensor(0.1511, grad_fn=<NllLossBackward>)\n","tensor(0.1243, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TNfRt7TQEKjl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3d8krnXEKjn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}